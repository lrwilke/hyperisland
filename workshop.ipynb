{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyper Island Data Analysis Workshop\n"
   ]
  },
  {
   "source": [
    "### Setup Linux\n",
    "\n",
    "On any distro that includes Python you should be able to just create a directory, `cd` into it and run the following commands:\n",
    "\n",
    "```\n",
    "python3 -m venv env\n",
    "source env/bin/activate\n",
    "python3 -m pip install -r requirements.txt\n",
    "```\n",
    "\n",
    "You can then run `jupyter notebook` to start the notebook server and access it from the browser using the URL that will be displayed in the terminal.\n",
    "\n",
    "The `matplotlib` library might cause some issues, I had to install it system-wide using:\n",
    "\n",
    "```\n",
    "sudo apt install python3-matplotlib\n",
    "```"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Setup Windows\n",
    "\n",
    "On Windows you need to make sure to install Python first. Get the latest version of Python 3 from https://www.python.org/downloads/windows/.\n",
    "This release will include the package manager `pip`.\n",
    "\n",
    "You should then create a folder and download the workshop files into it. The process of installing all dependencies should be done from the command line, using for example `cmd` or `PowerShell`. I opted for `cmd`, using the following commands to create and activate a virtual environment:\n",
    "\n",
    "```\n",
    "py -m venv env\n",
    "cd env/Scripts\n",
    "activate.bat\n",
    "cd ../..\n",
    "```\n",
    "\n",
    "At the beginning of your command prompt you should now read `(env)`, indicating that you activated this environment. You can now install the dependencies (provided that you downloaded the `requirements.txt` file into the folder you are using) with the following commands:\n",
    "\n",
    "```\n",
    "py -m pip install -r requirements\n",
    "py -m pip install notebook\n",
    "```\n",
    "\n",
    "`notebook` appears to be Windows-specific, so I'm not including it in the `requirements.txt` file.\n",
    "\n",
    "Once the installation finishes, you can start the notebook server using:\n",
    "\n",
    "```\n",
    "py -m notebook\n",
    "```\n",
    "\n",
    "Then go to the browser (it might open by itself, otherwise use the URL from the cmd-prompt) and create a new notebook for your analysis."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Getting Started: Imports I'll be Using\n",
    "\n",
    "The `requirements.txt` file gives you an idea of which libraries I'll be using for my work. You can import them as follows:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import datetime\n",
    "\n",
    "from pandas_datareader import data as pdr\n",
    "from bs4 import BeautifulSoup\n",
    "from scipy import stats"
   ]
  },
  {
   "source": [
    "### Data Source: Direct Download\n",
    "\n",
    "Instead of scraping the data by hand we are using a direct download from https://www.folkhalsomyndigheten.se/smittskydd-beredskap/utbrott/aktuella-utbrott/covid-19/statistik-och-analyser/bekraftade-fall-i-sverige/"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning and Preparation"
   ]
  },
  {
   "source": [
    "First step: Preview the data to manually identify any possible issues and get and idea of what the data look like.\n",
    "\n",
    "![Data preview](data.png)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### Loading and Combining Everything\n",
    "\n",
    "Assuming that possible errors or missing data are going to be somewhat consistent across the data, so not point in fixing that individually per file."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard utf-8 encoding does not work for Swedish\n",
    "# https://docs.python.org/3/library/codecs.html#standard-encodings\n",
    "c1 = pd.read_csv(\"data/Covid_Statistics_W6_10.csv\", encoding=\"Latin-1\")\n",
    "\n",
    "# 'display()' command is a part of IPython / Jupyter\n",
    "# use to achieve multiple outputs from one cell\n",
    "# don't use 'print()' inside display because of its None-return value\n",
    "display(f\"Number of rows: {len(c1.index)}\") # supports f-strings\n",
    "display(c1.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data into a dictionary of the form {'cN': DataFrame}\n",
    "data = dict()\n",
    "data['c1'] = c1\n",
    "\n",
    "data['c2'] = pd.read_csv(\"data/Covid_Statistics_W11_20.csv\", encoding=\"Latin-1\")\n",
    "data['c3'] = pd.read_csv(\"data/Covid_Statistics_W21_30.csv\", encoding=\"Latin-1\")\n",
    "data['c4'] = pd.read_csv(\"data/Covid_Statistics_W31_39.csv\", encoding=\"Latin-1\")\n",
    "data['c5'] = pd.read_csv(\"data/Covid_Statistics_W40_48.csv\", encoding=\"utf-8\")\n",
    "\n",
    "# concat takes an iterable and in case of dictionary orders by key\n",
    "df = pd.concat(data)\n",
    "\n",
    "display(f\"Number of rows: {len(df.index)}\")\n",
    "display(df.head())\n",
    "\n",
    "# looking at types to see where there might be data quality issues\n",
    "display(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Numeric Columns Imported as Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# finding which rows in a particular column are not numeric strings (only contain numbers)\n",
    "display(df['tot_antal_fall'][~df['tot_antal_fall'].str.isnumeric()])\n",
    "display(df['nya_fall_vecka'][df['nya_fall_vecka'].str.isnumeric() == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how to treat those values? let's take a look at summary statistics to get an idea\n",
    "# creating a new dataframe containing only the numeric data from those columns\n",
    "tot_antal_num = df['tot_antal_fall'][df['tot_antal_fall'].str.isnumeric()].apply(pd.to_numeric)\n",
    "nya_fall_num = df['nya_fall_vecka'][df['nya_fall_vecka'].str.isnumeric() == True].apply(pd.to_numeric)\n",
    "num_data = pd.DataFrame.from_dict({'tot_antal_fall': tot_antal_num, 'nya_fall_vecka': nya_fall_num})\n",
    "\n",
    "# using the 'aggregate' function to create specific summary statistics (subset of 'describe')\n",
    "num_data.agg({\"tot_antal_fall\": [\"min\", \"max\", \"median\"], \"nya_fall_vecka\": [\"min\", \"max\", \"median\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# handle NaN values first -> fill with 0 for visualization later\n",
    "df = df.fillna(0)\n",
    "\n",
    "# for 'total_antal_fall' let's use a random integer between 0 and 15\n",
    "df['tot_antal_fall'][~df['tot_antal_fall'].str.isnumeric()] = df['tot_antal_fall'][~df['tot_antal_fall'].str.isnumeric()].apply(lambda x: np.random.randint(0, 15))\n",
    "\n",
    "# something seems off about 'nya_fall_vecka', so let's just replace them all with 0\n",
    "df['nya_fall_vecka'][df['nya_fall_vecka'].str.isnumeric() == False] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the columns in the original dataframe numeric for further processing\n",
    "df['tot_antal_fall'] = df['tot_antal_fall'].astype(int)\n",
    "\n",
    "# first need to remove NaN values\n",
    "df['nya_fall_vecka'] = pd.to_numeric(df['nya_fall_vecka'], errors='coerce')\n",
    "df['nya_fall_vecka'] = df['nya_fall_vecka'].astype(int)\n",
    "\n",
    "display(df.dtypes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Handling Extreme Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there's extreme values in the nya_fall_vecka column\n",
    "df.plot.scatter(x='veckonummer', y='nya_fall_vecka')"
   ]
  },
  {
   "source": [
    "How do we address this issue? One approach is to exclude values that fall outside 2 or 3 standard deviations:\n",
    "\n",
    "![z-score](The_Normal_Distribution.svg)"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's remove extreme values in the 'nya_fall_vecka' column based on z-score\n",
    "# https://www.kite.com/python/answers/how-to-remove-outliers-from-a-pandas-dataframe-in-python\n",
    "df = df[(np.abs(stats.zscore(df.nya_fall_vecka)) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check if there are still extreme values present\n",
    "df.plot.scatter(x='veckonummer', y='nya_fall_vecka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's apply the same function again to remove the distinctly different value\n",
    "df = df[(np.abs(stats.zscore(df.nya_fall_vecka)) < 3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot.scatter(x='veckonummer', y='nya_fall_vecka')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare to 10948 rows in the beginning\n",
    "display(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spellchecking the 'Kommun' Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "# planning on grouping by 'kommune' since 'stadsdel' is too granular\n",
    "# need to check whether all the 'Kommun' are actually correct. So let's remove the statsdel and check the rest against a valid source\n",
    "df['Kommun'] = df.Kommun_stadsdel.apply(lambda x: x.split()[0])\n",
    "\n",
    "# Norwegian Wikipedia article on Swedish municipalities contains a table, so using that as a source\n",
    "source = 'https://no.wikipedia.org/wiki/Sveriges_kommuner#Kommunene:_M'\n",
    "\n",
    "# time to break out those web-scraping skills\n",
    "# don't forget to pip-install 'beautifulsoup4'!\n",
    "\n",
    "website = requests.get(source).text\n",
    "soup = BeautifulSoup(website,'lxml')\n",
    "\n",
    "tbl = soup.find('table')\n",
    "print(tbl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# but then again... pandas allows us to do that directly :)\n",
    "source_kommuner = pd.read_html(source)[0]\n",
    "\n",
    "# so let's use that and look for the values that are in our DF but not in 'source_kommuner'\n",
    "offenders = df.Kommun[df.Kommun.isin(source_kommuner.Kommune) == False]\n",
    "print(offenders.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement = dict.fromkeys(offenders, None)\n",
    "replacement['Falun'] = 'Falu'\n",
    "replacement['Gbg'] = 'Göteborg'\n",
    "replacement['Lilla'] = 'Lilla Edet' # this mistake is due to assuming kommune-names are always one word\n",
    "replacement['Malmös'] = 'Malmö'\n",
    "# replacement['Malung'] = 'Malung-Sälen' # this mistake is due to assuming kommune-names are always one word\n",
    "replacement['Skhlm'] = 'Stockholm'\n",
    "replacement['Stockholms'] = 'Stockholm'\n",
    "replacement['Upplands'] = 'Upplands Väsby' # this mistake is due to assuming kommune-names are always one word\n",
    "replacement['Östra'] = 'Östra Göinge' # this mistake is due to assuming kommune-names are always one word\n",
    "\n",
    "# could we find better way of extracting 'kommune' name?\n",
    "\n",
    "# replace wrong names with corrections in DF\n",
    "df['Kommun'] = df['Kommun'].map(replacement).fillna(df['Kommun'])\n",
    "\n",
    "# try to find 'offenders' again to see if it worked\n",
    "display(df.Kommun[df.Kommun.isin(source_kommuner.Kommune) == False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# export for use in Tableau\n",
    "df.to_csv(\"export.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Analysis\n",
    "\n",
    "Main analysis will be done visually in Tableau, but let's take a quick look to verify that the data makes sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's take a look at overall cases and new cases per week in two major cities\n",
    "df[df['Kommun'].isin(['Stockholm'])].groupby(['veckonummer', 'Kommun']).agg(\n",
    "    sum_new_cases=pd.NamedAgg(column=\"nya_fall_vecka\", aggfunc=sum),\n",
    "    sum_overall_cases=pd.NamedAgg(column=\"tot_antal_fall\", aggfunc=sum)\n",
    ").plot.line(subplots=True)\n",
    "\n",
    "df[df['Kommun'].isin(['Göteborg'])].groupby(['veckonummer', 'Kommun']).agg(\n",
    "    sum_new_cases=pd.NamedAgg(column=\"nya_fall_vecka\", aggfunc=sum),\n",
    "    sum_overall_cases=pd.NamedAgg(column=\"tot_antal_fall\", aggfunc=sum)\n",
    ").plot.line(subplots=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Pivot Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table can be more intuitive to work with\n",
    "# questions we might want to answer:\n",
    "# * are case numbers per inhabitant similar across municipalities?\n",
    "# * how have cases developed over time?\n",
    "# * ...\n",
    "\n",
    "# good step-by-step tutorial at https://pbpython.com/pandas-pivot-table-explained.html\n",
    "\n",
    "# simplest pivot is chosing an index; here taking two columns as index to group by municipality and week\n",
    "pd.pivot_table(df, index=[\"Kommun\", \"veckonummer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# selecting the columns that show numbers per inhabitant\n",
    "pd.pivot_table(df, index=[\"Kommun\", \"veckonummer\"], values=[\"antal_fall_per10000_inv\", \"tot_antal_fall_per10000inv\"], aggfunc=np.sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in order to do some more interesting work with the pivot table, let's add the country for each row\n",
    "# getting country information from https://en.wikipedia.org/wiki/List_of_municipalities_of_Sweden#List:_M\n",
    "\n",
    "# create new DF with county info\n",
    "source_counties = pd.read_html(\"https://en.wikipedia.org/wiki/List_of_municipalities_of_Sweden#List:_M\")[1]\n",
    "counties = source_counties[[\"Municipality\", \"County\"]]\n",
    "counties['Municipality'] = counties['Municipality'].str.replace(\" Municipality\", \"\")\n",
    "counties['County'] = counties['County'].str.replace(\" County\", \"\")\n",
    "\n",
    "# combine DFs so that county is included alongside municipality\n",
    "combined = pd.merge(df, counties, left_on=\"Kommun\", right_on=\"Municipality\", how=\"inner\").drop(\"Kommun\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_rows\", None)\n",
    "\n",
    "# pivot table for that combination\n",
    "pd.pivot_table(combined, index=[\"County\", \"Municipality\"], values=[\"antal_fall_per10000_inv\", \"tot_antal_fall_per10000inv\"], aggfunc=[np.sum, np.mean])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"max_rows\", 20)"
   ]
  },
  {
   "source": [
    "### Adding Finance Information\n",
    "\n",
    "Let's quickly add an additional data source to compare the Covid-data against by using Yahoo Finance.\n",
    "https://github.com/ranaroussi/yfinance"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OMX -> 30 most traded stocks at Stockholm stock exchange\n",
    "omx = yf.Ticker(\"^OMX\")\n",
    "omx_history = omx.history(period=\"11mo\")\n",
    "omx_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to use this in with pandas, so let's hijack the pandas datareader to use yfinance\n",
    "# this will make pdr output the data in a format we can use, but be faster because it uses yfinance\n",
    "yf.pdr_override()\n",
    "\n",
    "omx_data = pdr.get_data_yahoo(\"^OMX\", start=\"2020-01-01\", end=\"2020-12-07\")\n",
    "display(omx_data.head())\n",
    "display(omx_data.Close.plot())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df['Kommun'].isin(['Stockholm'])].groupby(['veckonummer', 'Kommun']).agg(\n",
    "    sum_new_cases=pd.NamedAgg(column=\"nya_fall_vecka\", aggfunc=sum)\n",
    ").plot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockholm = pd.DataFrame(df[df['Kommun'].isin(['Stockholm'])].groupby(['veckonummer', 'Kommun']).agg(\n",
    "    sum_new_cases=pd.NamedAgg(column=\"nya_fall_vecka\", aggfunc=sum),\n",
    "    veckonummer=pd.NamedAgg(column=\"veckonummer\", aggfunc=min)\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockholm['Date'] = stockholm.apply(lambda row: datetime.datetime.strptime(f\"2020-W{row.veckonummer}-1\", \"%Y-W%W-%w\"), axis=1)\n",
    "stockholm = stockholm.drop(\"veckonummer\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockholm = stockholm.set_index(\"Date\")\n",
    "omx_data = omx_data.reset_index()\n",
    "omx_data = omx_data.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stockholm.corrwith(omx_data.Close)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('env')",
   "metadata": {
    "interpreter": {
     "hash": "fe657483ec629b0b094c12c766eeb7f24fdd07d8ce206a0d390594c73f7d1c7f"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}